---
layout: post
title: "Bélády's Anomaly Doesn't Happen Often"
---

{{ page.title }}
================

<p class="meta">Anomaly is a really fun word. Try saying it ten times.</p>

It was 1969. The Summer of Love was raging, Hendrix was playing the anthem, and Forest Gump was running rampant. In New York, IBM researchers Bélády, Nelson, and Schedler were hot on the trail of something strange. They had a *paging machine*, a computer which kept its memory in *pages*, and sometimes moved those pages to storage. Weird<sup>[1](#foot1)</sup>. It wasn't only the machine that was weird, it was their performance results. Sometimes, giving the machine more memory made it slower. Without modern spook-hunting conveniences like Scooby Doo and Bill Murray, they had to hunt the ghost themselves.

What Bélády and team found is something now called [Bélády's anomaly](https://en.wikipedia.org/wiki/B%C3%A9l%C3%A1dy%27s_anomaly). In [their 1969 paper](https://dl.acm.org/doi/10.1145/363011.363155)<sup>[3](#foot3)</sup>, they describe it like this:

> Running on a paging machine and using the FIFO replacement algorithm, there are instances when the program runs faster if one reduces the storage space allotted to it.

More generally, this can happen with any FIFO cache: growing the cache can lead to worse results. This could, in theory, be a big problem for any tuning system or process which makes the assumption that growing the cache leads to better performance<sup>[2](#foot2)</sup>. This doesn't happen with LRU, LFU, and similar algorithms. Just with FIFO. Some point to Bélády's anomaly as a good reason for avoiding FIFO caches, even in systems where the reduced read-time coordination and space overhead would be a big win.

But how frequent is Bélády's anomaly really? Do we, as system builder, really need to avoid FIFO caches because of it?

One way to answer that question is how often we're likely to come across Bélády's anomaly purely by chance. It turns out that it doesn't happen very often at all. Starting with access patterns selected randomly with a uniform distribution of keys:

![](/blog/images/belady_freq_unif.png)

and with a Zipf distribution of keys:

![](/blog/images/belady_freq_zipf.png)

In each simulation here, we're comparing caches of size *N* and *N+1*, and counting the cases where the smaller cache has a superior hit rate. There are *N+2* unique pages in the system, the number that seems to maximize the frequency of the anomaly.

These really don't happen that frequently, with fewer than 0.175% of uniform access patterns showing the anomaly. Also, while the badness of the anomaly is [unbounded](https://arxiv.org/abs/1003.1336), none of these randomly-found strings show more than a small number of additional hits.

We're making the assumption that these random access patterns are representative of real-world access patterns. That's likely to be close to true in multi-tenant and multi-workload systems with large numbers of users or workloads, but may not be true in single-tenant single-workload database. There's also some risk that adversaries could construct strings which take advantage of this anomaly, but if that's possible it also seems possible for adversaries to create uncachable workloads more generally.

As a systems builder, Bélády's anomaly isn't a big concern to me. As somebody who appreciates a good edge case, I just love it.

**Footnotes**

1. <a name="foot1"></a> Weird enough that these days, paging is just *how computers work* for the vast majority of machines.
2. <a name="foot2"></a> As Cristina Abad [pointed out on Twitter](https://twitter.com/cabad3/status/1672071784328314880).
3. <a name="foot3"></a> Like a lot of 1960s systems work, this paper is a delight. It's easy to read, just five pages, and full of interesting analysis. Even the title, *An anomaly in space-time characteristics of certain programs running in a paging machine*, is delightful. Why can't systems papers like this get published anymore?